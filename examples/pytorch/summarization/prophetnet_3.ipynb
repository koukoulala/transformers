{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0128806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    "    XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration, XLMProphetNetConfig\n",
    ")\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "from preprocess_data import load_xglue\n",
    "import pickle\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb8c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ [\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856e2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    model_name_or_path = 'microsoft/xprophetnet-large-wiki100-cased-xglue-ntg'\n",
    "    cache_dir = \"/home/work/xiaoyu/ckpt/xprophtnet_ntg\"\n",
    "    use_fast_tokenizer = True\n",
    "    data_folder = \"/home/work/xiaoyu/datasets/xglue_full_dataset/sampled_NTG\"\n",
    "    pad_to_max_length = False\n",
    "    ignore_pad_token_for_loss = True\n",
    "    per_device_eval_batch_size = 2\n",
    "    val_max_target_length = None\n",
    "    max_target_length = 128\n",
    "    num_beams = 10\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f4248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/18/2021 22:08:50 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Use FP16 precision: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "logger.info(accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31163722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250012, 1024, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=args.use_fast_tokenizer, cache_dir=args.cache_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca00e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = accelerator.prepare(model)\n",
    "print(next(model.parameters()).device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efcf56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca7aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets_path = os.path.join(args.data_folder, \"processed_datasets.pkl\")\n",
    "tmp_file = open(processed_datasets_path, \"rb\")\n",
    "processed_datasets = pickle.load(tmp_file)\n",
    "tmp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce5a550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 of the test_dataset[fr] set: [1745, 30719, 18, 3435, 92872, 10851, 115, 36, 1822, 213677, 24, 350, 83863, 203322, 32, 27834, 2197, 264, 107, 36, 192142, 40578, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100].\n",
      "Sample 1 of the test_dataset[fr] set: [35188, 22797, 18, 126, 8943, 580, 2075, 16888, 2528, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100].\n",
      "Sample 2 of the test_dataset[fr] set: [1003, 48386, 115, 669, 45584, 835, 19, 740, 7221, 33, 10413, 17158, 264, 7281, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100].\n",
      "Sample -1 of the test_dataset[fr] set: [350, 36, 34650, 1692, 97, 4081, 2160, 19, 10282, 32231, 106, 9912, 126, 11949, 19, 57627, 15, 1851, 3249, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100].\n"
     ]
    }
   ],
   "source": [
    "test_dataset = {}\n",
    "lg = \"fr\"\n",
    "test_dataset[lg] = processed_datasets[\"test.\" + lg]\n",
    "index_list = [0, 1, 2, -1]\n",
    "for index in index_list:\n",
    "    input_tokens = test_dataset[lg][index]\n",
    "    print(f\"Sample {index} of the test_dataset[{lg}] set: {input_tokens['labels']}.\")\n",
    "    #input_sent = tokenizer.batch_decode(input_tokens[\"labels\"], skip_special_tokens=True)\n",
    "    #print(\"input_sent:\", \" \".join(input_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43272cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\" if args.pad_to_max_length else False\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34e1d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = {}\n",
    "test_dataloader[lg] = DataLoader(test_dataset[lg], collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "test_dataloader[lg] = accelerator.prepare(test_dataloader[lg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8930c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if args.val_max_target_length is None:\n",
    "    args.val_max_target_length = args.max_target_length\n",
    "gen_kwargs = {\n",
    "    \"max_length\": args.val_max_target_length if args is not None else config.max_length,\n",
    "    \"num_beams\": args.num_beams,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c95c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        #preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        #labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33b1344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbce366f5a64169b0375f98e775028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2635a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_seq Vice-présidente de l'Assemblée nationale, la macroniste Carole Bureau-Bonnard était chargée mardi après-midi d'animer la séance d'examen du projet de loi «confiance dans l'action publique». C'était sa\n",
      "decoded_preds [\"Carole Bureau-Bonnard, vice-présidente de l'Assemblée nationale, a connu une séance éprouvante\", \"Les plus grands fauteuils de l'île d'Antiparos\"]\n",
      "decoded_labels [\"Les débuts balbutiants d'une députée LREM provoque la pagaille à l'Assemblée nationale\", 'Ces maisons du sud qui nous inspirent']\n",
      "\n",
      "input_seq Le procès d'un Turc de 17 ans qui avait agressé en janvier 2016 à la machette un enseignant d'une école juive de Marseille portant une kippa, s'ouvre mercredi devant le tribunal pour enfants (TPE) de \n",
      "decoded_preds [',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', 'The S.O.A.A.D.:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,']\n",
      "decoded_labels ['Un jeune djihadiste de 17 ans en procès à Paris', 'Canada : la forme de ce nuage est invraisemblable']\n",
      "\n",
      "input_seq Face à l'inflation des médicaments, le Comité économique des produits de santé alerte les industriels, qui répondent coûts de recherche. Une fatalité? On la surnomme \"la pilule du président\", car elle\n",
      "decoded_preds ['Le Keytruda est un espoir pour les malades atteints de la tumeur de Jimmy Carter', 'Les voyageurs qui utilisent Android ou iOS seraient des voyageurs préférés']\n",
      "decoded_labels ['La vérité sur... la surenchère des anticancéreux', \"Dis-moi quel système d'exploitation mobile tu utilises, je te dirai quel voyageur tu es\"]\n",
      "\n",
      "input_seq La République serbe de Bosnie (Republika Srpska) s'est déclarée mercredi \"militairement neutre\" alors que le gouvernement central de Sarajevo, les Bosniaques musulmans et les Croates de Bosnie-Herzégo\n",
      "decoded_preds ['La République serbe de Bosnie déclarée \"militairement neutre\"', 'Les habitudes alimentaires des Français changent, selon une étude']\n",
      "decoded_labels ['La République serbe de Bosnie proclame sa neutralité militaire', 'Les Français de plus en plus adeptes du grignotage']\n",
      "\n",
      "input_seq Eva Longoria se livre dans une interview accordée à Hollywood Access au sujet de son mari, José Baston dont elle semble éperdument amoureuse. Grande supportrice de l'ex-candidate présidentielle Hillar\n",
      "decoded_preds [\"Eva Longoria s'est confiée sur le bonheur trouvé dans le bras de José Baston\", '3 exercices de respiration simples à mettre en oeuvre pour se détendre']\n",
      "decoded_labels ['Avec Pepe, Eva Longoria file le parfait amour', '3 exercices de respiration qui vont vous sauver en cas de coup de stress']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/18/2021 22:15:38 - INFO - __main__ - language fr results:\n",
      "08/18/2021 22:15:38 - INFO - __main__ - 3.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_seq Le kaki fait son grand come-back dans notre dressing. Par petites touches ou en total look, voici 20 tenues repérées sur Pinterest pour être stylée en kaki.. Un blouson satiné kaki avec une jupe fleur\n",
      "decoded_preds ['20 tenues pour être stylée en kaki', 'La tuerie de Las Vegas relance le débat sur le contrôle des armes à feu aux Etats-Unis']\n",
      "decoded_labels ['Pinterest : 20 façons de porter du kaki ce printemps', 'Fusillades: Les Etats-Unis pays développé le plus meurtrier au monde']\n",
      "{'score': 3.23696458177316, 'counts': [23, 9, 5, 3], 'totals': [249, 237, 225, 213], 'precisions': [9.236947791164658, 3.7974683544303796, 2.2222222222222223, 1.408450704225352], 'bp': 1.0, 'sys_len': 249, 'ref_len': 125}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for step, batch in enumerate(test_dataloader[lg]):\n",
    "    if step > 5:\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        #print(\"generated_tokens\", generated_tokens)\n",
    "\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        #print(\"generated_tokens\", generated_tokens)\n",
    "        \n",
    "        labels = batch[\"labels\"]\n",
    "        if not args.pad_to_max_length:\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "        labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        input_seq = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "        print(\"\\ninput_seq\", input_seq[0][:200])\n",
    "        print(\"decoded_preds\", decoded_preds)\n",
    "        print(\"decoded_labels\", decoded_labels)\n",
    "        \n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "        \n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "        \n",
    "res = metric.compute()\n",
    "print(res)\n",
    "results[lg] = round(res[\"score\"], 2)\n",
    "logger.info(f\"language {lg} results:\")\n",
    "logger.info(results[lg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2069f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
